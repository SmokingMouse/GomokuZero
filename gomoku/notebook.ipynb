{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "747456d5",
   "metadata": {},
   "source": [
    "# 1. 单局 PK 可视化区域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02731000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gomoku.player import ZeroMCTSPlayer, WrongZeroMCTSPlayer, IneffectiveZeroMCTSPlayer, play_one_game\n",
    "from gomoku.gomoku_env import GomokuEnvSimple, GomokuEnv\n",
    "from gomoku.policy import ZeroPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb9574a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  .  .  .  .  . \n",
      " 1 .  .  .  .  .  .  . \n",
      " 2 .  .  .  .  .  .  . \n",
      " 3 .  .  .  X  .  .  . \n",
      " 4 .  .  .  .  .  .  . \n",
      " 5 .  .  .  .  .  .  . \n",
      " 6 .  .  .  .  .  .  . \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  .  .  .  .  . \n",
      " 1 .  .  .  .  .  .  . \n",
      " 2 .  .  .  .  .  .  . \n",
      " 3 .  .  .  X  .  .  . \n",
      " 4 .  .  .  O  .  .  . \n",
      " 5 .  .  .  .  .  .  . \n",
      " 6 .  .  .  .  .  .  . \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  .  .  .  .  . \n",
      " 1 .  .  .  .  .  .  . \n",
      " 2 .  .  .  .  .  .  . \n",
      " 3 .  .  .  X  X  .  . \n",
      " 4 .  .  .  O  .  .  . \n",
      " 5 .  .  .  .  .  .  . \n",
      " 6 .  .  .  .  .  .  . \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  .  .  .  .  . \n",
      " 1 .  .  .  .  .  .  . \n",
      " 2 .  .  .  .  .  .  . \n",
      " 3 .  .  O  X  X  .  . \n",
      " 4 .  .  .  O  .  .  . \n",
      " 5 .  .  .  .  .  .  . \n",
      " 6 .  .  .  .  .  .  . \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  .  .  .  .  . \n",
      " 1 .  .  .  .  .  .  . \n",
      " 2 .  .  .  .  .  .  . \n",
      " 3 .  .  O  X  X  .  . \n",
      " 4 .  .  .  O  X  .  . \n",
      " 5 .  .  .  .  .  .  . \n",
      " 6 .  .  .  .  .  .  . \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  .  .  .  .  . \n",
      " 1 .  .  .  .  .  .  . \n",
      " 2 .  .  .  .  .  .  . \n",
      " 3 .  .  O  X  X  .  . \n",
      " 4 .  .  .  O  X  .  . \n",
      " 5 .  .  .  .  O  .  . \n",
      " 6 .  .  .  .  .  .  . \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  .  .  .  .  . \n",
      " 1 .  .  .  .  .  .  . \n",
      " 2 .  X  .  .  .  .  . \n",
      " 3 .  .  O  X  X  .  . \n",
      " 4 .  .  .  O  X  .  . \n",
      " 5 .  .  .  .  O  .  . \n",
      " 6 .  .  .  .  .  .  . \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  .  .  .  .  . \n",
      " 1 .  .  .  .  .  .  . \n",
      " 2 .  X  .  .  .  .  . \n",
      " 3 .  .  O  X  X  .  . \n",
      " 4 .  .  .  O  X  O  . \n",
      " 5 .  .  .  .  O  .  . \n",
      " 6 .  .  .  .  .  .  . \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  .  .  .  .  . \n",
      " 1 .  .  .  .  .  .  . \n",
      " 2 .  X  X  .  .  .  . \n",
      " 3 .  .  O  X  X  .  . \n",
      " 4 .  .  .  O  X  O  . \n",
      " 5 .  .  .  .  O  .  . \n",
      " 6 .  .  .  .  .  .  . \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  .  .  .  .  . \n",
      " 1 .  O  .  .  .  .  . \n",
      " 2 .  X  X  .  .  .  . \n",
      " 3 .  .  O  X  X  .  . \n",
      " 4 .  .  .  O  X  O  . \n",
      " 5 .  .  .  .  O  .  . \n",
      " 6 .  .  .  .  .  .  . \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  .  .  .  .  . \n",
      " 1 .  O  .  .  .  .  . \n",
      " 2 .  X  X  .  X  .  . \n",
      " 3 .  .  O  X  X  .  . \n",
      " 4 .  .  .  O  X  O  . \n",
      " 5 .  .  .  .  O  .  . \n",
      " 6 .  .  .  .  .  .  . \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  .  .  .  .  . \n",
      " 1 .  O  .  .  .  .  . \n",
      " 2 O  X  X  .  X  .  . \n",
      " 3 .  .  O  X  X  .  . \n",
      " 4 .  .  .  O  X  O  . \n",
      " 5 .  .  .  .  O  .  . \n",
      " 6 .  .  .  .  .  .  . \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  .  .  .  .  . \n",
      " 1 .  O  .  .  X  .  . \n",
      " 2 O  X  X  .  X  .  . \n",
      " 3 .  .  O  X  X  .  . \n",
      " 4 .  .  .  O  X  O  . \n",
      " 5 .  .  .  .  O  .  . \n",
      " 6 .  .  .  .  .  .  . \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  .  .  O  .  . \n",
      " 1 .  O  .  .  X  .  . \n",
      " 2 O  X  X  .  X  .  . \n",
      " 3 .  .  O  X  X  .  . \n",
      " 4 .  .  .  O  X  O  . \n",
      " 5 .  .  .  .  O  .  . \n",
      " 6 .  .  .  .  .  .  . \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  .  .  O  .  . \n",
      " 1 .  O  .  .  X  .  . \n",
      " 2 O  X  X  .  X  .  . \n",
      " 3 .  .  O  X  X  .  . \n",
      " 4 .  .  .  O  X  O  . \n",
      " 5 .  .  .  .  O  X  . \n",
      " 6 .  .  .  .  .  .  . \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  .  .  O  .  . \n",
      " 1 .  O  .  .  X  .  . \n",
      " 2 O  X  X  .  X  .  . \n",
      " 3 .  .  O  X  X  .  . \n",
      " 4 .  .  .  O  X  O  . \n",
      " 5 .  .  .  .  O  X  . \n",
      " 6 .  .  .  .  .  .  O \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  X  .  O  .  . \n",
      " 1 .  O  .  .  X  .  . \n",
      " 2 O  X  X  .  X  .  . \n",
      " 3 .  .  O  X  X  .  . \n",
      " 4 .  .  .  O  X  O  . \n",
      " 5 .  .  .  .  O  X  . \n",
      " 6 .  .  .  .  .  .  O \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  X  .  O  .  . \n",
      " 1 .  O  .  .  X  .  . \n",
      " 2 O  X  X  O  X  .  . \n",
      " 3 .  .  O  X  X  .  . \n",
      " 4 .  .  .  O  X  O  . \n",
      " 5 .  .  .  .  O  X  . \n",
      " 6 .  .  .  .  .  .  O \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  X  .  O  .  . \n",
      " 1 .  O  .  .  X  X  . \n",
      " 2 O  X  X  O  X  .  . \n",
      " 3 .  .  O  X  X  .  . \n",
      " 4 .  .  .  O  X  O  . \n",
      " 5 .  .  .  .  O  X  . \n",
      " 6 .  .  .  .  .  .  O \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  X  .  O  .  . \n",
      " 1 .  O  .  .  X  X  . \n",
      " 2 O  X  X  O  X  .  . \n",
      " 3 .  .  O  X  X  .  . \n",
      " 4 .  .  O  O  X  O  . \n",
      " 5 .  .  .  .  O  X  . \n",
      " 6 .  .  .  .  .  .  O \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  X  .  O  .  . \n",
      " 1 .  O  .  .  X  X  . \n",
      " 2 O  X  X  O  X  .  . \n",
      " 3 .  X  O  X  X  .  . \n",
      " 4 .  .  O  O  X  O  . \n",
      " 5 .  .  .  .  O  X  . \n",
      " 6 .  .  .  .  .  .  O \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  X  .  O  .  . \n",
      " 1 .  O  .  .  X  X  . \n",
      " 2 O  X  X  O  X  .  . \n",
      " 3 .  X  O  X  X  .  . \n",
      " 4 .  .  O  O  X  O  . \n",
      " 5 .  .  .  .  O  X  . \n",
      " 6 .  .  .  .  .  O  O \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  X  .  O  .  . \n",
      " 1 .  O  .  X  X  X  . \n",
      " 2 O  X  X  O  X  .  . \n",
      " 3 .  X  O  X  X  .  . \n",
      " 4 .  .  O  O  X  O  . \n",
      " 5 .  .  .  .  O  X  . \n",
      " 6 .  .  .  .  .  O  O \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  X  .  O  .  . \n",
      " 1 .  O  .  X  X  X  . \n",
      " 2 O  X  X  O  X  .  . \n",
      " 3 .  X  O  X  X  .  . \n",
      " 4 .  .  O  O  X  O  . \n",
      " 5 O  .  .  .  O  X  . \n",
      " 6 .  .  .  .  .  O  O \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  X  .  O  .  . \n",
      " 1 .  O  .  X  X  X  X \n",
      " 2 O  X  X  O  X  .  . \n",
      " 3 .  X  O  X  X  .  . \n",
      " 4 .  .  O  O  X  O  . \n",
      " 5 O  .  .  .  O  X  . \n",
      " 6 .  .  .  .  .  O  O \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  X  .  O  .  . \n",
      " 1 .  O  .  X  X  X  X \n",
      " 2 O  X  X  O  X  .  . \n",
      " 3 .  X  O  X  X  O  . \n",
      " 4 .  .  O  O  X  O  . \n",
      " 5 O  .  .  .  O  X  . \n",
      " 6 .  .  .  .  .  O  O \n",
      "\n",
      "   0  1  2  3  4  5  6\n",
      " 0 .  .  X  .  O  .  . \n",
      " 1 .  O  X  X  X  X  X \n",
      " 2 O  X  X  O  X  .  . \n",
      " 3 .  X  O  X  X  O  . \n",
      " 4 .  .  O  O  X  O  . \n",
      " 5 O  .  .  .  O  X  . \n",
      " 6 .  .  .  .  .  O  O \n",
      "\n",
      "函数 'play_one_game' 执行耗时: 4.0877 秒\n"
     ]
    }
   ],
   "source": [
    "from gomoku.player import IneffectiveZeroMCTSPlayer\n",
    "import torch\n",
    "\n",
    "board_size = 7\n",
    "\n",
    "policy = ZeroPolicy(board_size, num_blocks=2)\n",
    "# policy.load_state_dict(torch.load('continue_model/policy_step_940000.pth'))\n",
    "# policy.load_state_dict(torch.load('../models/gomoku_zero_9_pre5/policy_step_800000.pth'))\n",
    "policy.load_state_dict(torch.load('../models/gomoku_zero_9_lab_1/policy_step_80000.pth'))\n",
    "player1 = ZeroMCTSPlayer(policy)\n",
    "player2 = ZeroMCTSPlayer(policy)\n",
    "\n",
    "game = GomokuEnv(board_size)\n",
    "\n",
    "info = play_one_game(player1, player2, game=game,board_size=board_size, render=True, eager=True, itermax=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8977123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import torch\n",
    "import pstats\n",
    "import io\n",
    "\n",
    "# --- 开始性能分析 ---\n",
    "\n",
    "# 1. 创建一个 Profiler 对象\n",
    "profiler = cProfile.Profile()\n",
    "\n",
    "# 2. 启用 Profiler 并运行你的函数\n",
    "profiler.enable()\n",
    "policy = ZeroPolicy(board_size=15)\n",
    "policy.load_state_dict(torch.load('models/gomoku_zero_15_continue/policy_step_6000.pth'))\n",
    "player1 = ZeroMCTSPlayer(policy, itermax=200, device='cpu', eager=True)\n",
    "player2 = ZeroMCTSPlayer(policy, itermax=200, device='cpu', eager=True)\n",
    "\n",
    "game = GomokuEnv(15)\n",
    "\n",
    "info = play_one_game(player1, player2, board_size=15, render=True)\n",
    "\n",
    "profiler.disable()\n",
    "\n",
    "# 3. 创建一个 IO 流来捕获分析结果\n",
    "s = io.StringIO()\n",
    "\n",
    "# 4. 创建 pstats.Stats 对象来格式化和排序结果\n",
    "#    sort_stats() 的参数是排序依据，'cumulative' 是按累计耗时排序\n",
    "stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n",
    "\n",
    "# 5. 打印分析报告\n",
    "stats.print_stats()\n",
    "\n",
    "# 6. (可选) 只打印前 10 个最耗时的函数\n",
    "# stats.print_stats(10)\n",
    "\n",
    "print(s.getvalue())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987be24c",
   "metadata": {},
   "source": [
    "# 2. ARENA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "220a5295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangpeng.pada/GomokuZero/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-08 15:35:20,991\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from gomoku.player import arena_parallel\n",
    "from gomoku.player import ZeroMCTSPlayer, WrongZeroMCTSPlayer, IneffectiveZeroMCTSPlayer, play_one_game\n",
    "from gomoku.gomoku_env import GomokuEnvSimple\n",
    "from gomoku.policy import ZeroPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb3683f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel arena with 300 games on 8 CPUs...\n",
      "Arena finished!\n",
      "Player 1 wins: 133 (44.33%)\n",
      "Player 2 wins: 142 (47.33%)\n",
      "Draws: 25 (8.33%)\n",
      "函数 'arena_parallel' 执行耗时: 64.3855 秒\n"
     ]
    }
   ],
   "source": [
    "policy1 = ZeroPolicy(board_size)\n",
    "policy2 = ZeroPolicy(board_size)\n",
    "policy1.load_state_dict(torch.load('../models/gomoku_zero_9_lab_5/policy_step_100000.pth'))\n",
    "policy2.load_state_dict(torch.load('../models/gomoku_zero_9_lab_1/policy_step_100000.pth')) \n",
    "\n",
    "r = arena_parallel(\n",
    "    policy1,\n",
    "    policy2, \n",
    "    board_size=7,\n",
    "    num_cpus=8,\n",
    "    games=300,\n",
    "    itermax=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from worker import gather_selfplay_games\n",
    "\n",
    "\n",
    "gather_selfplay_games(\n",
    "    policy=ZeroPolicy(board_size=9),\n",
    "    device='cpu',\n",
    "    board_size=9,\n",
    "    num_workers=10,\n",
    "    games_per_worker=10,\n",
    "    itermax=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2380fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from player import self_play\n",
    "\n",
    "\n",
    "_ = self_play(\n",
    "    policy=ZeroPolicy(board_size=9),\n",
    "    device='cpu',\n",
    "    board_size=9,\n",
    "    itermax=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d930752",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_ = play_one_game(\n",
    "    player1=ZeroMCTSPlayer(ZeroPolicy(board_size=9)),\n",
    "    player2=ZeroMCTSPlayer(ZeroPolicy(board_size=9)),\n",
    "    board_size=9,\n",
    "    # render=True,\n",
    "    game=GomokuEnvSimple(board_size=9),\n",
    "    itermax=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08988b6d",
   "metadata": {},
   "source": [
    "# 3. 评价棋局"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c3d5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangpeng.pada/GomokuZero/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-07 16:22:06,128\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from gomoku.player import ZeroMCTSPlayer, WrongZeroMCTSPlayer, IneffectiveZeroMCTSPlayer, play_one_game, self_play\n",
    "from gomoku.gomoku_env import GomokuEnvSimple\n",
    "from gomoku.policy import ZeroPolicy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a49aa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数 'play_one_game' 执行耗时: 7.5808 秒\n"
     ]
    }
   ],
   "source": [
    "board_size = 9\n",
    "\n",
    "policy = ZeroPolicy(board_size)\n",
    "policy.load_state_dict(torch.load('../models/gomoku_zero_9_pre5/policy_step_710000.pth'))\n",
    "\n",
    "device='cpu'\n",
    "player1 = ZeroMCTSPlayer(policy, device=device) \n",
    "player2 = ZeroMCTSPlayer(policy, device=device)\n",
    "\n",
    "env = GomokuEnvSimple(board_size)\n",
    "\n",
    "winner, infos = play_one_game(\n",
    "    player1, player2, board_size=board_size, game=env,\n",
    "    itermax=200, eager=False\n",
    ")\n",
    "# print(f\"Game over! Winner: {winner}\")\n",
    "# return infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d56846ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infos = self_play(policy, 'cpu', board_size=board_size, itermax=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b87dda4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(states, index):\n",
    "    import numpy as np\n",
    "    states_tensor = torch.from_numpy(np.array(infos['states'], dtype=np.float32))\n",
    "    probs, winrate = policy(states_tensor)\n",
    "\n",
    "    symbols = {0: '.', 1: 'X', 2: 'O'}\n",
    "    board_str = \"  \" + \" \".join([f\"{i:2d}\" for i in range(9)]) + \"\\n\"\n",
    "    \n",
    "    for i in range(9):\n",
    "        cells = []\n",
    "        for j in range(9):\n",
    "            if states[index][ 0, i, j] ==  1:  \n",
    "                cell = 1\n",
    "            elif states[index][ 1, i, j] == 1:\n",
    "                cell = 2\n",
    "            else:\n",
    "                cell = 0\n",
    "            cells.append(cell)\n",
    "        board_str += f\"{i:2d} \" + \" \".join([f\"{symbols[cell]} \" for cell in cells]) + \"\\n\"\n",
    "    print(board_str)\n",
    "    print(winrate[index])\n",
    "    values, indices = torch.topk(torch.softmax(probs, dim=-1)[index], 10)\n",
    "\n",
    "    def action_2_index(action):\n",
    "        return action // 9, action % 9\n",
    "\n",
    "    for i  in range(len(indices)):\n",
    "        a, b = action_2_index(indices[i].item())\n",
    "        print(f\"[{a:2d},{b:2d}]:{values[i].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7576ac5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2  3  4  5  6  7  8\n",
      " 0 .  .  .  .  .  .  .  .  . \n",
      " 1 .  .  .  .  .  .  .  .  . \n",
      " 2 .  .  .  .  .  .  .  .  . \n",
      " 3 .  .  .  .  .  .  O  .  . \n",
      " 4 .  .  .  X  .  .  .  .  . \n",
      " 5 .  .  .  .  .  .  .  .  . \n",
      " 6 .  .  .  .  .  .  .  .  . \n",
      " 7 .  .  .  .  .  .  .  .  . \n",
      " 8 .  .  .  .  .  .  .  .  . \n",
      "\n",
      "tensor([0.7832], grad_fn=<SelectBackward0>)\n",
      "[ 4, 4]:0.9994\n",
      "[ 5, 4]:0.0002\n",
      "[ 3, 4]:0.0001\n",
      "[ 3, 5]:0.0001\n",
      "[ 3, 3]:0.0001\n",
      "[ 5, 3]:0.0000\n",
      "[ 5, 5]:0.0000\n",
      "[ 4, 2]:0.0000\n",
      "[ 4, 6]:0.0000\n",
      "[ 2, 3]:0.0000\n"
     ]
    }
   ],
   "source": [
    "render(infos['states'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5367e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "render(info[1]['states'], -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33853096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_2_index(action):\n",
    "    return action // 9, action % 9\n",
    "\n",
    "probs = torch.tensor(info[1]['probs'])\n",
    "values, indices = probs[-2].topk(10)\n",
    "\n",
    "for i  in range(len(indices)):\n",
    "    a, b = action_2_index(indices[i].item())\n",
    "    print(f\"[{a:2d},{b:2d}]:{values[i].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2c5110",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WebSocket.__init__() missing 3 required positional arguments: 'environ', 'socket', and 'rfile'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 高手只需运行这个脚本就能让他们的 AI 和你的对战\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwebsocket\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m ws = \u001b[43mwebsocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mWebSocket\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m ws.connect(\u001b[33m\"\u001b[39m\u001b[33mws://localhost:8000/ws/play\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m ws.send(\u001b[33m'\u001b[39m\u001b[33m{\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhandshake\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbot_name\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDeepSeeker_V1\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m}\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: WebSocket.__init__() missing 3 required positional arguments: 'environ', 'socket', and 'rfile'"
     ]
    }
   ],
   "source": [
    "# 高手只需运行这个脚本就能让他们的 AI 和你的对战\n",
    "import websocket\n",
    "\n",
    "ws = websocket.WebSocket()\n",
    "ws.connect(\"ws://localhost:8000/ws/play\")\n",
    "ws.send('{\"type\": \"handshake\", \"bot_name\": \"DeepSeeker_V1\"}')\n",
    "# ... 循环接收棋盘，计算，发送落子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4e221c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([50, 3, 9, 9])\n",
      "y shape: torch.Size([50, 9, 9])\n",
      "\n",
      "Sample Board (Channel 0 - Self):\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0.]])\n",
      "\n",
      "Sample Board (Channel 1 - Opponent):\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "Sample Board (Channel 2 - Last Move):\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "Target:\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def generate_benchmark_dataset():\n",
    "    \"\"\"\n",
    "    生成 50 个合法的五子棋残局。\n",
    "    修正点：\n",
    "    1. 增加干扰子(D)以平衡黑白棋子数量，确保局面合法 (Count(Black) == Count(White) or +1)。\n",
    "    2. 明确指定 last_move_coord，且必须对应棋盘上的 O。\n",
    "    \"\"\"\n",
    "    dataset_X = []\n",
    "    dataset_y = []\n",
    "    \n",
    "    board_size = 9 # 使用 9x9 进行测试\n",
    "    \n",
    "    def parse_board(pattern, target_coords, last_move_coord):\n",
    "        # 初始化通道\n",
    "        self_plane = np.zeros((board_size, board_size), dtype=np.float32) # 我方 (X)\n",
    "        oppo_plane = np.zeros((board_size, board_size), dtype=np.float32) # 对手 (O)\n",
    "        last_plane = np.zeros((board_size, board_size), dtype=np.float32) # 对手上一手\n",
    "        target_plane = np.zeros((board_size, board_size), dtype=np.float32)\n",
    "\n",
    "        rows = pattern.strip().split('\\n')\n",
    "        # 去除每行的首尾空格\n",
    "        rows = [r.strip() for r in rows]\n",
    "        \n",
    "        for r, row_str in enumerate(rows):\n",
    "            for c, char in enumerate(row_str):\n",
    "                if char == 'X': # 我方棋子\n",
    "                    self_plane[r, c] = 1.0\n",
    "                elif char == 'O': # 对手棋子（关键子）\n",
    "                    oppo_plane[r, c] = 1.0\n",
    "                elif char == 'D': # 对手棋子（干扰子，用来平衡数量的）\n",
    "                    oppo_plane[r, c] = 1.0\n",
    "\n",
    "        # 设置 Last Move (必须是对手下的)\n",
    "        if last_move_coord:\n",
    "            if oppo_plane[last_move_coord[0], last_move_coord[1]] != 1.0:\n",
    "                print(f\"Warning: Last move {last_move_coord} is not on an Opponent stone!\")\n",
    "            last_plane[last_move_coord[0], last_move_coord[1]] = 1.0\n",
    "\n",
    "        # 设置目标概率\n",
    "        prob = 1.0 / len(target_coords)\n",
    "        for tr, tc in target_coords:\n",
    "            target_plane[tr, tc] = prob\n",
    "            \n",
    "        # 校验数量平衡 (假设自己是黑棋先手，轮到自己下，那么当前盘面 黑==白 或 黑==白-1 都是不对的)\n",
    "        # 轮到 Self 下：\n",
    "        # 如果 Self 是黑(先)：盘面应该是 Black == White\n",
    "        # 如果 Self 是白(后)：盘面应该是 Black == White + 1 (即 Opponent 多一个)\n",
    "        # 为了简化，我们默认生成的局面都是：轮到 Self 走，且双方子数大致平衡\n",
    "        \n",
    "        x_tensor = np.stack([self_plane, oppo_plane, last_plane])\n",
    "        return x_tensor, target_plane\n",
    "\n",
    "    def augment_and_add(x, y):\n",
    "        # 旋转翻转扩充数据 (1 -> 8)\n",
    "        x_trans = np.transpose(x, (1, 2, 0)) \n",
    "        for k in range(4):\n",
    "            x_rot = np.rot90(x_trans, k)\n",
    "            y_rot = np.rot90(y, k)\n",
    "            dataset_X.append(np.transpose(x_rot, (2, 0, 1)))\n",
    "            dataset_y.append(y_rot)\n",
    "            \n",
    "            x_flip = np.fliplr(x_rot)\n",
    "            y_flip = np.fliplr(y_rot)\n",
    "            dataset_X.append(np.transpose(x_flip, (2, 0, 1)))\n",
    "            dataset_y.append(y_flip)\n",
    "\n",
    "    # ================== 题目设计 ==================\n",
    "\n",
    "    # --- Level 1: 嵌五必杀 (Immediate Win) ---\n",
    "    # 场景：轮到 X 下。X 有 4 个子，O 也有 4 个子。\n",
    "    # X 在中间空了一个洞 (0,1,2,3,4 中的 2 是空的)\n",
    "    # 这是一个平衡局：X:4, O:4 (D是凑数的 O)\n",
    "    l1_pattern = \"\"\"\n",
    "    .........\n",
    "    DD.......\n",
    "    .O.......\n",
    "    .XX.XX...\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    \"\"\" \n",
    "    # 分析：X有4个，O有3个(D+D+O)。为了平衡，O必须是4个。\n",
    "    # 假设对手刚下了 (2, 1) 的 O，试图防守，但没防住下面\n",
    "    l1_targets = [(3, 3)]\n",
    "    x, y = parse_board(l1_pattern, l1_targets, last_move_coord=(2, 1))\n",
    "    augment_and_add(x, y) # +8\n",
    "\n",
    "    # --- Level 2: 必须堵冲四 (Critical Defense) ---\n",
    "    # 场景：轮到 X 下。O 连了 4 个 (冲四)，X 必须堵。\n",
    "    # 盘面：O 有 4 个，X 应该有 3 个或 4 个。\n",
    "    l2_pattern = \"\"\"\n",
    "    .........\n",
    "    ....O....\n",
    "    ....O....\n",
    "    X...O....\n",
    "    X...O....\n",
    "    X........\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    \"\"\"\n",
    "    # 分析：O 竖线 4 个，X 左边 3 个。O 刚下了 (1, 4) 形成冲四。\n",
    "    # 此时 O=4, X=3。合法（白多黑少，轮到黑下）。\n",
    "    l2_targets = [(0, 4), (5, 4)] # 两头都要防，或者防一头，这里假设两头均分\n",
    "    x, y = parse_board(l2_pattern, l2_targets, last_move_coord=(1, 4))\n",
    "    augment_and_add(x, y) # +8 = 16\n",
    "\n",
    "    # --- Level 3: 活三进攻 (Active 3) ---\n",
    "    # 场景：X 形成活三，准备下一步成活四。\n",
    "    # 盘面：X 有 3 个，O 有 3 个。\n",
    "    l3_pattern = \"\"\"\n",
    "    .........\n",
    "    D.D......\n",
    "    .........\n",
    "    .XXX.....\n",
    "    ..O......\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    \"\"\"\n",
    "    # X: 3个, O: 3个 (D, D, O)。平衡。\n",
    "    # O 刚下在 (4, 2) 挡路，但没挡住上面。\n",
    "    l3_targets = [(3, 0), (3, 4)] # 左右延伸\n",
    "    x, y = parse_board(l3_pattern, l3_targets, last_move_coord=(4, 2))\n",
    "    augment_and_add(x, y) # +8 = 24\n",
    "\n",
    "    # --- Level 4: 复杂防守 (VCF 防守) ---\n",
    "    # 场景：O 形成了 \"跳活三\" (O O . O)，中间那个点如果不补就是死。\n",
    "    # 盘面：O: 3个, X: 2个。\n",
    "    l4_pattern = \"\"\"\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    ..O.O.O..\n",
    "    ....X....\n",
    "    ....X....\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    \"\"\"\n",
    "    # O: 3个, X: 2个。合法（白3黑2，轮到黑）。\n",
    "    # O 刚下了 (3, 6) 的 O。\n",
    "    l4_targets = [(3, 3), (3, 5), (3, 7)] # (3,5)是填中间，最优；(3,3)/(3,7)是堵两头\n",
    "    # 为了简化，我们只要求堵中间\n",
    "    l4_targets = [(3, 5)] \n",
    "    x, y = parse_board(l4_pattern, l4_targets, last_move_coord=(3, 6))\n",
    "    augment_and_add(x, y) # +8 = 32\n",
    "\n",
    "    # --- Level 5: 开局 (Opening) ---\n",
    "    # 场景 A: 空棋盘。黑棋先手。\n",
    "    # Last Move 是 None (全0)\n",
    "    l5_a_pattern = \"\"\"\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    \"\"\"\n",
    "    l5_a_targets = [(4, 4)]\n",
    "    # 特殊处理：last_move_coord 设为 None\n",
    "    x, y = parse_board(l5_a_pattern, l5_a_targets, last_move_coord=None) \n",
    "    # 这是一个特殊的 Case，手动处理 last plane 全 0\n",
    "    x[2] = np.zeros((board_size, board_size)) \n",
    "    \n",
    "    dataset_X.append(x)\n",
    "    dataset_y.append(y)\n",
    "    \n",
    "    # 场景 B: 必应点 (Standard Response)\n",
    "    # 黑下了天元，白下了直指 (3, 4)。轮到黑下。\n",
    "    # X:1, O:1。\n",
    "    l5_b_pattern = \"\"\"\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    ....O....\n",
    "    ....X....\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    \"\"\"\n",
    "    # X: (4,4), O: (3,4)。O 刚下的 (3,4)。\n",
    "    # 常用点：(5,5), (3,5), (5,3)\n",
    "    l5_b_targets = [(5, 5), (3, 5), (5, 3)]\n",
    "    x, y = parse_board(l5_b_pattern, l5_b_targets, last_move_coord=(3, 4))\n",
    "    augment_and_add(x, y) # +8 = 41\n",
    "\n",
    "    # 补齐剩余数据 (用 Level 2 的变体 - 仅仅是方向不同)\n",
    "    l_supp_pattern = \"\"\"\n",
    "    .........\n",
    "    ..X......\n",
    "    ..X.O....\n",
    "    ..X.O....\n",
    "    ....O....\n",
    "    ....O....\n",
    "    .........\n",
    "    .........\n",
    "    .........\n",
    "    \"\"\"\n",
    "    # O 竖线4个，X 竖线3个。O 刚下了 (5, 4)。\n",
    "    l_supp_targets = [(2, 4), (6, 4)]\n",
    "    x, y = parse_board(l_supp_pattern, l_supp_targets, last_move_coord=(5, 4))\n",
    "    augment_and_add(x, y) # +8 = 49\n",
    "    \n",
    "    # 凑第50个，复制一下空棋盘\n",
    "    dataset_X.append(dataset_X[32])\n",
    "    dataset_y.append(dataset_y[32])\n",
    "\n",
    "    X_tensor = torch.tensor(np.array(dataset_X[:50]), dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(np.array(dataset_y[:50]), dtype=torch.float32)\n",
    "\n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X, y = generate_benchmark_dataset()\n",
    "    print(\"X shape:\", X.shape) # 应为 (50, 3, 9, 9)\n",
    "    print(\"y shape:\", y.shape) # 应为 (50, 9, 9)\n",
    "    \n",
    "    # 验证一下数据是否正常\n",
    "    sample_idx = 10 # 随便取一个\n",
    "    print(\"\\nSample Board (Channel 0 - Self):\")\n",
    "    print(X[sample_idx][0])\n",
    "    print(\"\\nSample Board (Channel 1 - Opponent):\")\n",
    "    print(X[sample_idx][1])\n",
    "    print(\"\\nSample Board (Channel 2 - Last Move):\")\n",
    "    print(X[sample_idx][2])\n",
    "    print(\"\\nTarget:\")\n",
    "    print(y[sample_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e4f4738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def Evaluate(poliy, X, y):\n",
    "    logits = policy(X)[0]\n",
    "\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    print(probs)\n",
    "    y = y.reshape(y.shape[0],-1)\n",
    "    cse = torch.sum(y * probs, dim=1).sum() # 先在每个样本上求和，再在batch上求平均\n",
    "\n",
    "    # print(cse)\n",
    "    return cse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ac61d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m Evaluate(\u001b[43mpolicy\u001b[49m, X, y)\n",
      "\u001b[31mNameError\u001b[39m: name 'policy' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate(policy, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd1c46e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.5245, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = ZeroPolicy(9, num_blocks=2)\n",
    "# policy.load_state_dict(torch.load('continue_model/policy_step_940000.pth'))\n",
    "policy.load_state_dict(torch.load('../models/gomoku_zero_9_pre5/policy_step_100000.pth'))\n",
    "Evaluate(policy, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6069991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0393, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gomoku.policy import ZeroPolicy\n",
    "from gomoku.evaluate import Evaluate\n",
    "\n",
    "policy = ZeroPolicy(7, num_blocks=2)\n",
    "# policy.load_state_dict(torch.load('continue_model/policy_step_940000.pth'))\n",
    "# policy.load_state_dict(torch.load('../models/gomoku_zero_9_pre5/policy_step_950000.pth'))\n",
    "# policy.load_state_dict(torch.load('continue_model/policy_step_940000.pth'))\n",
    "Evaluate(policy, board_size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "241c590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y[:1]\n",
    "# X[:1]\n",
    "# X[-1]\n",
    "for i in range(50):\n",
    "    if X[i][0].sum() == 0:\n",
    "        print(i)\n",
    "X[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc3837a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gomokuzero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
